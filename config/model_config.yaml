modeling: 
# --- tree & boosting schedule -------------------------------------
  n_estimators:      600          # total boosting rounds
  learning_rate:     0.05         # “eta”
  max_depth:         6            # tree depth (controls interactions)
  min_child_weight:  1            # min sum Hessian in a node
  subsample:         0.8          # row sampling per tree
  colsample_bytree:  0.8          # feature sampling per tree
  # --- regularisation -----------------------------------------------
  reg_alpha:         0.0          # L1 penalty
  reg_lambda:        1.0          # L2 penalty
  # --- misc ----------------------------------------------------------
  objective:         reg:squarederror
  booster:           gbtree
  n_jobs:            -1           # use all cores
  random_state:      42           # reproducibility

training:
  # champion windows chosen by clv_grid_search_autotune.py
  history_months: 9
  pred_months: 6

  # hold-out & early-stopping settings for final training
  test_size: 0.20
  early_stopping_rounds: 20
  eval_metric: rmse

  # optional back-test override; otherwise ranking script writes last_score_cutoff
  cutoff_date: "2021-06-30"

composite_weights:
  rmse: 0.5
  r2:   0.5

scoring:
  percentile_bins: [0, 0.01, 0.05, 0.10, 0.20, 1.0]
  segment_labels: ["Top 1%", "Top 5%", "Top 10%", "Top 20%", "Others"]

clustering:
  n_clusters: 5
  random_state: 42
  clustering_algorithm: kmeans

monitoring:
  drift_threshold_rmse: 0.20   # retrain if RMSE ↑ 20 %
  drift_threshold_r2:  0.15    # retrain if R²  ↓ 15 %

# populated automatically by score_and_rank_customers.py
run:
  last_score_cutoff: "2022-03-30"        

general:
  timestamp_format: "%Y%m%d_%H%M%S"
